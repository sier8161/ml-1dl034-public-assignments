{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet classification with naive bayes\n",
    "\n",
    "For this notebook we are going to implement a naive bayes classifier for classifying positive or negative based on the words in the tweet. Recall that for two events A and B the bayes theorem says\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "where P(A) and P(B) is the ***class probabilities*** and P(B|A) is called ***conditional probabilities***. this gives us the probability of A happening, given that B has occurred. So as an example if we want to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we will obtain the following \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\" in tweet}) = \\frac{P(\"\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\"\\text{\"good\" in tweet})} $$\n",
    "\n",
    "This means that to find the probability of \"is this a positive tweet given that it contains the word \"good\" \" we need the probability of \"good\" being in a positive tweet, the probability of a tweet being positive and the probability of \"good\" being in a tweet. \n",
    "\n",
    "Similarly, if we want to obtain the opposite \"is this a negative tweet given that it contains the word \"boring\" \"\n",
    "we get \n",
    "\n",
    "$$ P(\\text{\"negative\"}|\\text{\"boring\" in tweet}) = \\frac{P(\\text{\"boring\" in tweet}|\\text{\"negative\"})P(\\text{\"negative\"})}{P(\\text{\"boring\" in tweet})} $$\n",
    "\n",
    "where we need the probability of \"boring\" being in a negative tweet, the probability of a tweet negative being and the probability of \"boring\" being in a tweet. \n",
    "\n",
    "We can now build a classifier where we compare those two probabilities and whichever is the larger one it's classified as \n",
    "\n",
    "if P(\"positive\"|\"good\" in tweet) $>$ P(\"negative\"|\"boring\" in tweet)\n",
    "    \n",
    "   Tweet is positive\n",
    "\n",
    "else\n",
    "   \n",
    "   Tweet is negative\n",
    "\n",
    "Now let's expand this to handle multiple features and put the Naive assumption into bayes theorem. This means that if features are independent we have \n",
    "\n",
    "$$ P(A,B) = P(A)P(B) $$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{P(b_1|A)P(b_2|A)...P(b_n|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ P(A|b_1,b_2,...,b_n) = \\frac{\\prod_i^nP(b_i|A)P(A)}{P(b_1)P(b_2)...P(b_n)} $$\n",
    "\n",
    "\n",
    "So with our previous example expanded with more words \"is this a positive tweet given that it contains the word \"good\" and \"interesting\" \" gives us \n",
    "\n",
    "$$ P(\\text{\"positive\"}|\\text{\"good\", \"interesting\" in tweet}) = \\frac{P(\\text{\"good\" in tweet}|\\text{\"positive\"})P(\\text{\"interesting\" in tweet}|\\text{\"positive\"})P(\\text{\"positive\"})}{P(\\text{\"good\" in tweet})P(\\text{\"interesting\" in tweet})} $$\n",
    "\n",
    "As you can see the denominator remains constant which means we can remove it and the final classifier end up\n",
    "\n",
    "$$y = argmax_A P(A)\\prod_i^nP(b_i|A) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:37:14.983555600Z",
     "start_time": "2024-01-07T13:37:13.641527800Z"
    }
   },
   "outputs": [],
   "source": [
    "# stuff to import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:59:08.449137900Z",
     "start_time": "2024-01-07T13:59:07.974610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>processed_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@zoevermillion i think you should get this ins...</td>\n",
       "      <td>zoevermillion think get instead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>@IdolScott packing is a drag...I always overpa...</td>\n",
       "      <td>idolscott packing dragi always overpack feel f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>oh, best believe ima get that money honey!! nite</td>\n",
       "      <td>best believe ima get money honey nite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Now i'm going to bed, gotta wake up early tomo...</td>\n",
       "      <td>ow going bed got ta wake early tomorrow take b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>just got back from shopping!!!! got loads of D...</td>\n",
       "      <td>ust got back shopping got load dvd make hannah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199946</th>\n",
       "      <td>199995</td>\n",
       "      <td>0</td>\n",
       "      <td>Royce and Bentley have eaten my #Bold</td>\n",
       "      <td>oyce bentley eaten bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199947</th>\n",
       "      <td>199996</td>\n",
       "      <td>1</td>\n",
       "      <td>feels a little better today - lots of vitamin ...</td>\n",
       "      <td>eel little better today lot vitamin back normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199948</th>\n",
       "      <td>199997</td>\n",
       "      <td>1</td>\n",
       "      <td>i am content</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199949</th>\n",
       "      <td>199998</td>\n",
       "      <td>1</td>\n",
       "      <td>so yeah. extreme rules, my baby won.</td>\n",
       "      <td>yeah extreme rule baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199950</th>\n",
       "      <td>199999</td>\n",
       "      <td>0</td>\n",
       "      <td>i have game cds on my desk, but just dont feel...</td>\n",
       "      <td>game cd desk dont feel like playing game comin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199951 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  sentiment  \\\n",
       "0                0          1   \n",
       "1                1          1   \n",
       "2                2          1   \n",
       "3                3          1   \n",
       "4                4          1   \n",
       "...            ...        ...   \n",
       "199946      199995          0   \n",
       "199947      199996          1   \n",
       "199948      199997          1   \n",
       "199949      199998          1   \n",
       "199950      199999          0   \n",
       "\n",
       "                                                    tweet  \\\n",
       "0       @zoevermillion i think you should get this ins...   \n",
       "1       @IdolScott packing is a drag...I always overpa...   \n",
       "2       oh, best believe ima get that money honey!! nite    \n",
       "3       Now i'm going to bed, gotta wake up early tomo...   \n",
       "4       just got back from shopping!!!! got loads of D...   \n",
       "...                                                   ...   \n",
       "199946             Royce and Bentley have eaten my #Bold    \n",
       "199947  feels a little better today - lots of vitamin ...   \n",
       "199948                                      i am content    \n",
       "199949              so yeah. extreme rules, my baby won.    \n",
       "199950  i have game cds on my desk, but just dont feel...   \n",
       "\n",
       "                                         processed_tweets  \n",
       "0                         zoevermillion think get instead  \n",
       "1       idolscott packing dragi always overpack feel f...  \n",
       "2                   best believe ima get money honey nite  \n",
       "3       ow going bed got ta wake early tomorrow take b...  \n",
       "4       ust got back shopping got load dvd make hannah...  \n",
       "...                                                   ...  \n",
       "199946                            oyce bentley eaten bold  \n",
       "199947    eel little better today lot vitamin back normal  \n",
       "199948                                            content  \n",
       "199949                             yeah extreme rule baby  \n",
       "199950  game cd desk dont feel like playing game comin...  \n",
       "\n",
       "[199951 rows x 4 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('data_for_theoretical_notebook_1.csv', encoding='latin')\n",
    "tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets split the data into a training set and a test set using scikit-learns train_test_split function \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T13:37:59.025504100Z",
     "start_time": "2024-01-07T13:37:59.020482200Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets[\"processed_tweets\"]\n",
    "tweets_labels = tweets[\"sentiment\"]\n",
    "\n",
    "# Split data into train_tweets, test_tweets, train_labels and test_labels\n",
    "train_tweets, test_tweets, train_labels, test_labels = train_test_split(\n",
    "    tweets_data, tweets_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to build our classifier is \"probability of positive tweet\" P(pos) , \"probability of negative tweet\" P(neg), \"probability of word in tweet given tweet is positive\" P(w|pos) and \"probability of word in tweet given tweet is negative\" P(w|neg). Start by calculating the probability that a tweet is positive and negative respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pos = train_labels.mean()\n",
    "P_neg = 1-P_pos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For P(w|pos), P(w|neg) we need to count how many tweets each word occur in. Count the number of tweets each word occurs in and store in the word counter. An entry in the word counter is for instance {'good': 'Pos':150, 'Neg': 10} meaning good occurs in 150 positive tweets and 10 negative tweets. Be aware that we are not interested in calculating multiple occurrences of the same word in the same tweet. Also, we change the labels from 0 for \"Negative\" and 1 for \"Positive\" to \"Neg\" and \"Pos\" respectively.For each word convert it to lower case. You can use Python's [lower](https://www.w3schools.com/python/ref_string_lower.asp). Another handy Python string method is [split](https://www.w3schools.com/python/ref_string_split.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_labels = train_labels.replace(0, \"Neg\", regex=True)\n",
    "final_train_labels = new_train_labels.replace(1, \"Pos\", regex=True)\n",
    "word_counter = {}\n",
    "for (tweet, label) in zip(train_tweets, final_train_labels):\n",
    "    words = list(set(tweet.lower().split()))\n",
    "    for word in words:\n",
    "        if word not in word_counter:\n",
    "            word_counter[word] = {'Pos': 0, 'Neg': 0}\n",
    "        word_counter[word][label] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with a smaller subset of words just to save up some time. Find the 1500 most occuring words in tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_of_words_to_use = 1500\n",
    "popular_words = sorted(word_counter.items(),\n",
    "                       key=lambda x: x[1]['Pos'] + x[1]['Neg'], reverse=True)\n",
    "popular_words = [x[0] for x in popular_words[:nr_of_words_to_use]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute P(w|pos), P(w|neg) for the popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w_given_pos = {}\n",
    "P_w_given_neg = {}\n",
    "\n",
    "P_pos_given_w = {}  # To solve this the bayesian way\n",
    "total_words = sum(sum(word_counter[word][label]\n",
    "                  for word in word_counter) for label in [\"Pos\", \"Neg\"])\n",
    "\n",
    "for word in popular_words:\n",
    "    if word not in word_counter:\n",
    "        P_pos_given_w[word] = {\"Pos\": 0.5}\n",
    "        count = 0\n",
    "    else:\n",
    "        labels = word_counter[word]\n",
    "        pos = labels[\"Pos\"]\n",
    "        neg = labels[\"Neg\"]\n",
    "        count = pos+neg\n",
    "        if count == 0:\n",
    "            P_pos_given_w[word] = {\"Pos\": 0.5}\n",
    "        else:\n",
    "            P_pos_given_w[word] = {\"Pos\": pos/count}\n",
    "\n",
    "    P_w = count/total_words\n",
    "\n",
    "    P_w_given_pos[word] = (P_pos_given_w[word][\"Pos\"]*P_w)/P_pos\n",
    "    P_w_given_neg[word] = ((1 - P_pos_given_w[word][\"Pos\"])*P_w)/P_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = {\n",
    "    'basis': popular_words,\n",
    "    'P(pos)': P_pos,\n",
    "    'P(neg)': P_neg,\n",
    "    'P(w|pos)': P_w_given_pos,\n",
    "    'P(w|neg)': P_w_given_neg\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tweet_classifier function that takes your trained classifier and a tweet and returns wether it's about Positive or Negative using the popular words selected. Note that if there are words in the basis words in our classifier that are not in the tweet we have the opposite probabilities i.e P(w_1 occurs )* P(w_2 does not occur) * .... if w_1 occurs and w_2 does not occur. The function should return wether the tweet is Positive or Negative. i.e 'Pos' or 'Neg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_classifier(tweet, classifier_dict):\n",
    "    \"\"\" param tweet: string containing tweet message\n",
    "        param classifier: dict containing 'basis' - training words\n",
    "                                          'P(pos)' - class probabilities\n",
    "                                          'P(neg)' - class probabilities\n",
    "                                          'P(w|pos)' - conditional probabilities\n",
    "                                          'P(w|neg)' - conditional probabilities\n",
    "\n",
    "        return: either 'Pos' or 'Neg'\n",
    "    \"\"\"\n",
    "# ... Code for classifying tweets using the naive bayes classifier\n",
    "    prob_pos = classifier_dict[\"P(pos)\"]\n",
    "    prob_neg = classifier_dict[\"P(neg)\"]\n",
    "\n",
    "    test_words = list(set(tweet.lower().split()))\n",
    "    for train_word in classifier_dict[\"basis\"]:\n",
    "        if train_word in test_words:\n",
    "            prob_pos *= classifier_dict[\"P(w|pos)\"][train_word]\n",
    "            prob_neg *= classifier_dict[\"P(w|neg)\"][train_word]\n",
    "        else:\n",
    "            prob_pos *= (1 - classifier_dict[\"P(w|pos)\"][train_word])\n",
    "            prob_neg *= (1 - classifier_dict[\"P(w|neg)\"][train_word])\n",
    "\n",
    "    if prob_pos > prob_neg:\n",
    "        return \"Pos\"\n",
    "    else:\n",
    "        return \"Neg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(classifier, test_tweets, test_labels):\n",
    "    total = len(test_tweets)\n",
    "    correct = 0\n",
    "    for (tweet, label) in zip(test_tweets, test_labels):\n",
    "        predicted = tweet_classifier(tweet, classifier)\n",
    "        if predicted == label:\n",
    "            correct = correct + 1\n",
    "    return (correct/total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_labels = test_labels.replace(0, \"Neg\", regex=True)\n",
    "final_test_labels = new_test_labels.replace(1, \"Pos\", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7326\n"
     ]
    }
   ],
   "source": [
    "acc = test_classifier(classifier, test_tweets, final_test_labels)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional work\n",
    "\n",
    "In basic sentiment analysis classifications we have 3 classes \"Positive\", \"Negative\" and \"Neutral\". Although because it is challenging to create the \"Neutral\" class. Try to improve the accuracy by filtering the dataset from the perspective of removing words that indicate neutrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
